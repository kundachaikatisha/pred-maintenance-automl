{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.core.workspace import Workspace\n",
        "from azureml.core.dataset import Dataset\n",
        "from azureml.train.automl import AutoMLConfig"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Connect to the workspace\n",
        "ws = Workspace.from_config()"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color='blue'> Challenge 4</font>:\n",
        "Get the compute cluster that you created in the previous lab and complete the cell below.\n",
        "1. Define a variable for your cluster name\n",
        "2. Verify that the cluster does not exist already. If the cluster doesn't exist, create one.\n",
        "\n",
        "Tips: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=python"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Choose a name for your CPU cluster\n",
        "cpu_cluster_name = \"\"\n",
        "\n",
        "# Verify that the cluster does not exist already. If it doesn't exist, create one. \n",
        "'''\n",
        "Code goes here \n",
        "'''"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color='blue'> Challenge 5</font>:\n",
        "In the previous lab, you pre-processed the data that we going are to use for the training and registered it as a dataset. Now you need to use the dataset for training. Write a single line of code below that gets the training dataset.\n",
        "\n",
        "Tips: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-version-track-datasets#:~:text=An%20Azure%20Machine%20Learning%20dataset.%20Register%20and%20retrieve,a%20specific%20version%20by%20name%20and%20version%20number."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the dataset that we prepared earlier\n",
        "'''\n",
        "Code goes here\n",
        "'''"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "'\\nCode goes here\\n'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automl Configuration\n",
        "\n",
        "Instantiate a AutoMLConfig object. This defines the settings and data used to run the experiment.\n",
        "\n",
        "|Property|Description|\n",
        "|-|-|\n",
        "|**task**|classification or regression or forecasting|\n",
        "|**primary_metric**|This is the metric that you want to optimize. Classification supports the following primary metrics: <br><i>accuracy</i><br><i>AUC_weighted</i><br><i>average_precision_score_weighted</i><br><i>norm_macro_recall</i><br><i>precision_score_weighted</i>|\n",
        "|**iteration_timeout_minutes**|Time limit in minutes for each iteration.|\n",
        "|**blocked_models** | *List* of *strings* indicating machine learning algorithms for AutoML to avoid in this run. <br><br> Allowed values for **Classification**<br><i>LogisticRegression</i><br><i>SGD</i><br><i>MultinomialNaiveBayes</i><br><i>BernoulliNaiveBayes</i><br><i>SVM</i><br><i>LinearSVM</i><br><i>KNN</i><br><i>DecisionTree</i><br><i>RandomForest</i><br><i>ExtremeRandomTrees</i><br><i>LightGBM</i><br><i>GradientBoosting</i><br><i>TensorFlowDNN</i><br><i>TensorFlowLinearClassifier</i><br><br>Allowed values for **Regression**<br><i>ElasticNet</i><br><i>GradientBoosting</i><br><i>DecisionTree</i><br><i>KNN</i><br><i>LassoLars</i><br><i>SGD</i><br><i>RandomForest</i><br><i>ExtremeRandomTrees</i><br><i>LightGBM</i><br><i>TensorFlowLinearRegressor</i><br><i>TensorFlowDNN</i><br><br>Allowed values for **Forecasting**<br><i>ElasticNet</i><br><i>GradientBoosting</i><br><i>DecisionTree</i><br><i>KNN</i><br><i>LassoLars</i><br><i>SGD</i><br><i>RandomForest</i><br><i>ExtremeRandomTrees</i><br><i>LightGBM</i><br><i>TensorFlowLinearRegressor</i><br><i>TensorFlowDNN</i><br><i>Arima</i><br><i>Prophet</i>|\n",
        "|**allowed_models** |  *List* of *strings* indicating machine learning algorithms for AutoML to use in this run. Same values listed above for **blocked_models** allowed for **allowed_models**.|\n",
        "|**experiment_exit_score**| Value indicating the target for *primary_metric*. <br>Once the target is surpassed the run terminates.|\n",
        "|**experiment_timeout_hours**| Maximum amount of time in hours that all iterations combined can take before the experiment terminates.|\n",
        "|**enable_early_stopping**| Flag to enble early termination if the score is not improving in the short term.|\n",
        "|**featurization**| 'auto' / 'off'  Indicator for whether featurization step should be done automatically or not. Note: If the input data is sparse, featurization cannot be turned on.|\n",
        "|**n_cross_validations**|Number of cross validation splits.|\n",
        "|**training_data**|Input dataset, containing both features and label column.|\n",
        "|**label_column_name**|The name of the label column.|"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color='blue'> Challenge 6</font>:\n",
        "1. Choose an appropriate primary metric to use for the auto ml training and why. Include this primary metric into **auto_settings** below as `primary_metric = <value>`\n",
        "\n",
        "Tips: _You can learn more about primary metrics_ [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train#primary-metric)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "automl_settings = {\n",
        "    \"experiment_timeout_hours\" : 0.3,\n",
        "    \"enable_early_stopping\" : False,\n",
        "    \"iteration_timeout_minutes\": 5,\n",
        "    \"max_concurrent_iterations\": 4,\n",
        "    \"max_cores_per_iteration\": -1,\n",
        "    \"n_cross_validations\": 3,\n",
        "    \"featurization\": 'auto',\n",
        "    \"verbosity\": logging.INFO,\n",
        "}\n",
        "\n",
        "automl_config = AutoMLConfig(task = 'classification',\n",
        "                             debug_log = 'automl_errors.log',\n",
        "                             compute_target=compute_target,\n",
        "                             blocked_models = ['KNN','LinearSVM'],\n",
        "                             enable_onnx_compatible_models=True,\n",
        "                             training_data = training_ds,\n",
        "                             label_column_name = 'label1',\n",
        "                             **automl_settings\n",
        "                            )"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Models\n",
        "\n",
        "Call the `submit` method on the experiment object and pass the run configuration. Execution of local runs is synchronous. Depending on the data and the number of iterations this can run for a while.\n",
        "In this example, we specify `show_output = True` to print currently running iterations to the console."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = 'pdm-automl'\n",
        "project_folder = './sample_projects/pdm-automl'\n",
        "\n",
        "experiment = Experiment(ws, experiment_name)"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "remote_run = experiment.submit(automl_config, show_output = False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Submitting remote run.\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>pdm-automl</td><td>AutoML_7336b233-2830-4548-9d93-893ff2323647</td><td>automl</td><td>NotStarted</td><td><a href=\"https://ml.azure.com/runs/AutoML_7336b233-2830-4548-9d93-893ff2323647?wsid=/subscriptions/321dcb27-13a1-47b1-9ea3-c51d0eb1617e/resourcegroups/pdm-rg/workspaces/pdm-ws&amp;tid=72f988bf-86f1-41af-91ab-2d7cd011db47\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore the Results"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Widget for Monitoring Runs\n",
        "\n",
        "The widget will first report a \"loading\" status while running the first iteration. After completing the first iteration, an auto-updating graph and table will be shown. The widget will refresh once per minute, so you should see the graph update as child runs complete.\n",
        "\n",
        "**Note:** The widget displays a link at the bottom. Use this link to open a web interface to explore the individual run details."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.widgets import RunDetails\n",
        "from azureml.core.run import Run\n",
        "RunDetails(remote_run).show() "
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1644478321277
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "best_run = remote_run.get_best_child()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = best_run.properties[\"model_name\"]\n",
        "\n",
        "script_file_name = \"inference/score.py\"\n",
        "\n",
        "best_run.download_file(\"outputs/scoring_file_v_1_0_0.py\", \"inference/score.py\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "description = \"AutoML Model for predictive maintenance\"\n",
        "tags = None\n",
        "model = remote_run.register_model(\n",
        "    model_name=model_name, description=description, tags=tags\n",
        ")\n",
        "\n",
        "print(\n",
        "    remote_run.model_id\n",
        ")  # This will be written to the script file later in the notebook."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy\n",
        "\n",
        "### Retrieve the Best Model\n",
        "\n",
        "Below we select the best pipeline from our iterations.  The `get_output` method returns the best run and the fitted model. Overloads on `get_output` allow you to retrieve the best run and fitted model for *any* logged metric or for a particular *iteration*."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.webservice import AciWebservice\n",
        "from azureml.core.model import Model\n",
        "\n",
        "inference_config = InferenceConfig(environment = best_run.get_environment(), entry_script=script_file_name)\n",
        "\n",
        "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 2, \n",
        "                                               memory_gb = 2, \n",
        "                                               tags = {'type': \"automl_classification\"}, \n",
        "                                               description = 'service for Automl PDM Classification')\n",
        "\n",
        "aci_service_name = 'automl-remote-pdm'\n",
        "print(aci_service_name)\n",
        "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
        "aci_service.wait_for_deployment(True)\n",
        "print(aci_service.state)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import azureml\n",
        "from azureml.core import Workspace, Run\n",
        "from azureml.core.model import Model\n",
        "\n",
        "print(\"Azure ML: \", azureml.core.VERSION)\n",
        "ws = Workspace.from_config()\n",
        "#model = Model(ws, \"model_name\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Logs from a Deployed Web Service\n",
        "\n",
        "Gets logs from a deployed web service."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test and Evaluate on Holdout Set\n",
        "\n",
        "Now that the model is trained, run the test data through the trained model to get the predicted values.  This calls the ACI web service to do the prediction.\n",
        "\n",
        "Note that the JSON passed to the ACI web service is an array of rows of data.  Each row should either be an array of values in the same order that was used for training or a dictionary where the keys are the same as the column names used for training.  The example below uses dictionary rows."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# build test data matrix\n",
        "df_mean = test_df[lag_cols].rolling(window=lag_window).mean()\n",
        "df_std = test_df[lag_cols].rolling(window=lag_window).std()\n",
        "df_mean.columns = ['MA'+s for s in lag_cols]\n",
        "df_std.columns = ['STD'+s for s in lag_cols]\n",
        "df_test = pd.concat([test_df,df_mean,df_std], axis=1, join='inner')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#cut head by id, due to lagging transformation\n",
        "#train_array = [df_train[df_train['id']==id].values[lag_window+40:,:] for id in df_train['id'].unique()]\n",
        "\n",
        "test_arrayx=df_test[0:0]\n",
        "for id in df_test['id'].unique():\n",
        "    dfy=df_test[df_test['id']==id].iloc[-1:]\n",
        "    test_arrayx=test_arrayx.append(dfy)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# build the matrix\n",
        "test_x = test_arrayx.drop(test_arrayx.iloc[:, :4], axis = 1)\n",
        "test_y = test_arrayx['label1']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "X_test_json = test_x.to_json(orient='records')\n",
        "data = \"{\\\"data\\\": \" + X_test_json +\"}\"\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "resp = requests.post(aci_service.scoring_uri, data, headers=headers)\n",
        "\n",
        "y_pred = json.loads(json.loads(resp.text))['result']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "actual = list(test_y)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "cf =confusion_matrix(actual,y_pred)\n",
        "plt.imshow(cf,cmap=plt.cm.Blues,interpolation='nearest')\n",
        "plt.colorbar()\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "class_labels = ['no','yes']\n",
        "tick_marks = np.arange(len(class_labels))\n",
        "plt.xticks(tick_marks,class_labels)\n",
        "plt.yticks([-0.5,0,1,1.5],['','no','yes',''])\n",
        "# plotting text value inside cells\n",
        "thresh = cf.max() / 2.\n",
        "for i,j in itertools.product(range(cf.shape[0]),range(cf.shape[1])):\n",
        "    plt.text(j,i,format(cf[i,j],'d'),horizontalalignment='center',color='white' if cf[i,j] >thresh else 'black')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "# Compute fpr, tpr, thresholds and roc auc\n",
        "fpr, tpr, thresholds = roc_curve(actual,y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
        "plt.ylabel('True Positive Rate or (Sensitivity)')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.6 - AzureML",
      "language": "python",
      "name": "python3-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}